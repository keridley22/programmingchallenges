{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLCSU Lead Data Scientist challenge\n",
    "### Katherine Ridley, 25/11/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ‘Lead Data Scientist’ position at the Health Economics Unit requires a thorough understanding of data science, analytics, machine learning and programming. The successful candidate should have a passion for using analytics to better understand healthcare data and to improve decision making.\n",
    "\n",
    "As part of the interview process we would like you to analyse a dataset which has information on heart failure. The data contains various other relevant healthcare variables, for example, relating to patient demographics and diseases.\n",
    "\n",
    "We want you to process the data using any technique(s) of your choosing, for example:\n",
    "\n",
    "Regression\n",
    "Classification\n",
    "Clustering\n",
    "Forecasting\n",
    "Causal inference\n",
    "Statistical analysis\n",
    "Other machine learning/deep learning techniques\n",
    "You can use any programming language in the analysis. We then want you to write up the work in no more than two pages.\n",
    "\n",
    "Please send your code and report by the date mentioned is your test invite to david.sgorbati@nhs.net\n",
    "\n",
    "This is your opportunity to showcase your skills and to demonstrate that you would be suitable for this role in our team.\n",
    "\n",
    "We very much look forward to receiving and reviewing your submission, and we wish you the best of luck in this assignment.\n",
    "\n",
    "Best wishes HEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import packages\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from docx import Document\n",
    "from docx2pdf import convert\n",
    "from docx.shared import Inches\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import shapiro\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import anderson\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn import tree \n",
    "from sklearn.tree import export_graphviz\n",
    "from io import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import BaggingClassifier as BGC\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.models import CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral6\n",
    "from bokeh.models import (BasicTicker, ColorBar, ColumnDataSource,\n",
    "                          LinearColorMapper, PrintfTickFormatter)\n",
    "\n",
    "from bokeh.transform import transform\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "from bokeh.palettes import RdYlGn6, RdYlGn9\n",
    "from bokeh.sampledata.autompg import autompg\n",
    "from bokeh.sampledata.unemployment1948 import data\n",
    "\n",
    "from bokeh.sampledata.commits import data\n",
    "from bokeh.transform import jitter\n",
    "from bokeh.transform import factor_cmap, factor_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "\n",
    "## To run thise code, add path to 'Test_data.csv' here:\n",
    "\n",
    "path = \"C:/Users/KatherineRidley/Documents/Challenges/\"\n",
    "\n",
    "df = pd.read_csv(path + \"Test_data.csv\")\n",
    "\n",
    "\n",
    "##make directories\n",
    "experiments = ['datasets','descriptives', 'correlations', 'analysis of variance', 'regression', 'classification']\n",
    "for experiment in experiments:\n",
    "    os.makedirs(path+experiment, exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data handling\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "print('Unique values in each column: \\n', df.apply(lambda col: col.unique()))\n",
    "print('\\nNumber of null values in each column:\\n',df.apply(lambda col: col.isnull().sum()))\n",
    "\n",
    "print('\\nNumber of unique values in each column: \\n', df.apply(lambda col: len(col.unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##identify outliers in linear variables\n",
    "\n",
    "#for l in linear:\n",
    " #   print(l, df[l].describe())\n",
    "  #  print('\\n')\n",
    "\n",
    "##remove outliers in linear variables\n",
    "\n",
    "#z score outlier removal for bmi\n",
    "#df = df[(np.abs(stats.zscore(df['BMI'])) < 3)]\n",
    "\n",
    "\n",
    "#for l in linear:\n",
    "   # print(l, df[l].describe())\n",
    "   # print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical key:\n",
    "#### BMIcategory\n",
    "Underweight: Less than 18.5.\n",
    "\n",
    "Optimum range: 18.5 to 24.9.\n",
    "\n",
    "Overweight: 25 to 29.9.\n",
    "\n",
    "Class I obesity: 30 to 34.9.\n",
    "\n",
    "Class II obesity: 35 to 39.9.\n",
    "\n",
    "Class III obesity: More than 40.\n",
    "\n",
    "Class IIII obesity: More than 60.\n",
    "\n",
    "##### Covid:\n",
    "0 = pre-2020\n",
    "\n",
    "1 = 2020-\n",
    "\n",
    "### Dummies key:\n",
    "\n",
    "Race_simplified: 'White': 1 , Other combined: 2\n",
    "\n",
    "Race:  'White': 1, 'Black': 2, 'Asian': 3, 'Hispanic':4, 'American Indian/Alaskan Native':5, 'Other': 6\n",
    "\n",
    "Sex: 'Female': 1, 'Male': 2\n",
    "\n",
    "GenHealth: 'Excellent': 1, 'Very Good': 2, 'Good': 3, 'Fair':4, 'Poor':5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.categorical = list(df.select_dtypes(include=['object']).columns)\n",
    "        self.linear = list(df.select_dtypes(include=['int64','float64']).columns)\n",
    "        self.df_clean = df.copy()\n",
    "        self.df_clean = self.df_clean.drop(['Unnamed: 0'], axis=1)\n",
    "    def clean_categorical(self):\n",
    "        df = self.df_clean\n",
    "        categorical = self.categorical\n",
    "        #for c in categorical:\n",
    "            #print(c, df[c].value_counts())\n",
    "        df.loc[(df.GenHealth=='goo_d') | (df.GenHealth=='Goo_d'),'GenHealth'] = 'Good'\n",
    "        df.loc[(df.GenHealth=='ExCellent'),'GenHealth'] = 'Excellent'\n",
    "        df.loc[df.AgeCategory=='80 or older','AgeCategory'] = '80+'\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "        df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "        df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "        df['dayofweek'] = pd.DatetimeIndex(df['date']).dayofweek\n",
    "        ##age cateogry order\n",
    "        df['AgeCategory'] = pd.Categorical(df['AgeCategory'], categories=['18-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+'], ordered=True)\n",
    "        df['GenHealth'] = pd.Categorical(df['GenHealth'], categories=['Poor', 'Fair', 'Good', 'Very good', 'Excellent'], ordered=True)\n",
    "            \n",
    "        \n",
    "        self.categorical.remove('date')\n",
    "        return df, self.categorical\n",
    "    def clean_linear(self, df):\n",
    "        df = self.df_clean\n",
    "        \n",
    "        #for l in linear:\n",
    "            #print(l, df[l].describe())\n",
    "            #print('\\n')\n",
    "        #df = df[(np.abs(stats.zscore(df['BMI'])) < 3)]\n",
    "        df['BMIcategory'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, 60, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese I', 'Obese II', 'Obese III', 'Obese IIII'])\n",
    "        df['BMIcategory'] = df['BMIcategory'].astype('category')\n",
    "        self.categorical.append('BMIcategory')\n",
    "        self.linear.remove('Unnamed: 0')\n",
    "        self.linear.remove('Patient_ID')\n",
    "        \n",
    "        \n",
    "        return df, self.categorical, self.linear\n",
    "\n",
    "    def dummies(self, df_clean):\n",
    "        yesnovar = ['Dead/Alive', 'SkinCancer', 'HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'PhysicalActivity', 'Asthma', 'KidneyDisease']\n",
    "\n",
    "        for v in yesnovar:\n",
    "\n",
    "\n",
    "            df_clean[v] = df_clean[v].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "        df_clean['Sex#'] = df_clean['Sex'].replace({'Female': 1, 'Male': 2})\n",
    "\n",
    "        df_clean['Race#'] = df_clean['Race'].replace({'White': 1, 'Black': 2, 'Asian': 3, 'Hispanic':4, 'American Indian/Alaskan Native':5, 'Other': 6})\n",
    "\n",
    "        df_clean['GenHealth#'] = df_clean['GenHealth'].replace({'Excellent': 1, 'Very good': 2, 'Good': 3, 'Fair':4, 'Poor':5})\n",
    "\n",
    "        df_clean['Diabetic#'] = df_clean['Diabetic'].replace({'Yes': 1, 'No': 0, 'Yes (during pregnancy)': 1, 'No, borderline diabetes': 0})\n",
    "\n",
    "        df_clean['Age_Group'] = df_clean['AgeCategory'].str[:2].astype(int)\n",
    "\n",
    "        df_clean['Race_simplified'] = df_clean['Race#'].replace({3:2, 4:2, 5:2, 6:2})\n",
    "\n",
    "        df_clean.loc[df_clean['year']<2020, 'Covid'] = 0\n",
    "        df_clean.loc[df_clean['year']>=2020, 'Covid'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        df_clean_dummies = df_clean.drop(['Race', 'Sex', 'GenHealth', 'AgeCategory', 'Diabetic', 'date', 'BMIcategory', 'Patient_ID'], axis=1)\n",
    "        \n",
    "        return df_clean_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##run data cleaner\n",
    "\n",
    "dc = DataCleaner(df)\n",
    "df_clean, categorical = dc.clean_categorical()\n",
    "\n",
    "df_clean, categorical, linear = dc.clean_linear(df_clean)\n",
    "\n",
    "datetime = ['year', 'month', 'dayofweek']\n",
    "\n",
    "df_clean_dummies = dc.dummies(df_clean)\n",
    "\n",
    "#for d in datetime:\n",
    "    #print(d, df_clean[d].value_counts())\n",
    "exp = 'datasets'\n",
    "df_clean.to_csv(path + exp + '/df_clean.csv', index=False)\n",
    "df_clean_dummies.to_csv(path + exp + '/df_clean_dummies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Desciptive statistics and plots\n",
    "\n",
    "class Descriptives:\n",
    "    def __init__(self, df, categorical, linear, datetime):\n",
    "        self.df = df\n",
    "        self.categorical = categorical\n",
    "        self.linear = linear\n",
    "        self.datetime = datetime\n",
    "        self.exp = 'descriptives'\n",
    "    def make_descriptives(self):\n",
    "        df = self.df\n",
    "        categorical = self.categorical\n",
    "        linear = self.linear\n",
    "        for c in categorical:\n",
    "            print(c, df[c].value_counts())\n",
    "            print('\\n')\n",
    "        for l in linear:\n",
    "            print(l, df[l].describe())\n",
    "            print('\\n')\n",
    "    def make_plots(self):\n",
    "        df = self.df\n",
    "        categorical = self.categorical\n",
    "        linear = self.linear\n",
    "        for c in categorical:\n",
    "            df[c].value_counts().plot(kind='bar')\n",
    "            plt.title(c)\n",
    "            plt.show()\n",
    "        for l in linear:\n",
    "            df[l].plot(kind='hist')\n",
    "            plt.title(l)\n",
    "            plt.show()\n",
    "            df[l].plot(kind='box')\n",
    "            plt.title(l)\n",
    "            plt.show()\n",
    "\n",
    "    def categorical_counts(self):\n",
    "        cats = len(self.categorical)\n",
    "\n",
    "        ##if cats = odd, add 1 to make it even\n",
    "        if cats % 2 != 0:\n",
    "            max = math.floor((cats + 1)/4)\n",
    "        else:\n",
    "            max = math.floor(cats/4)\n",
    "\n",
    "        sns.set_context(\"paper\", font_scale=1.3, rc={\"lines.linewidth\": 2.5})\n",
    "        fig, axs = plt.subplots(4, max, figsize=(20, 20))\n",
    "\n",
    "        axs = axs.ravel()\n",
    "        axs=axs.flatten()\n",
    "        for i in range(cats):\n",
    "            \n",
    "            cat = categorical[i]\n",
    "\n",
    "            sns.countplot(x=cat, data=self.df, ax=axs[i])\n",
    "            axs[i].set_title(cat, fontsize=20)\n",
    "            axs[i].set_xlabel('')\n",
    "            axs[i].set_ylabel('Count')\n",
    "            axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.exp + '/categoricalcounts.png')\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "    def linear_counts(self):\n",
    "        lin = len(self.linear)\n",
    "\n",
    "        ##if lin = odd, add 1 to make it even\n",
    "        if lin % 2 != 0:\n",
    "            max = math.floor((lin + 1)/4)\n",
    "        else:\n",
    "            max = math.floor(lin/4)\n",
    "\n",
    "        sns.set_context(\"paper\", font_scale=1.3, rc={\"lines.linewidth\": 2.5})\n",
    "        fig, axs = plt.subplots(4, max, figsize=(20, 20))\n",
    "\n",
    "        axs = axs.ravel()\n",
    "        axs=axs.flatten()\n",
    "        for i in range(lin):\n",
    "            \n",
    "            lin = linear[i]\n",
    "\n",
    "            sns.histplot(self.df[lin], ax=axs[i], kde=True)\n",
    "            axs[i].set_title(lin, fontsize=20)\n",
    "            \n",
    "          \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.exp + '/lineardist.png')\n",
    "        plt.show()\n",
    "\n",
    "    def datetimehistplots(self):\n",
    "        df = self.df\n",
    "        datetime = self.datetime\n",
    "        \n",
    "        sns.set_context(\"paper\", font_scale=1.3, rc={\"lines.linewidth\": 2.5})\n",
    "        fig, axs = plt.subplots(1,3, figsize=(20, 4))\n",
    "\n",
    "        axs = axs.ravel()\n",
    "        axs=axs.flatten()\n",
    "        for i in range(3):\n",
    "            \n",
    "            dt = datetime[i]\n",
    "\n",
    "            sns.histplot(self.df[dt], ax=axs[i])\n",
    "            axs[i].set_title(dt, fontsize=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.exp + '/datetimehist.png')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##run descriptives\n",
    "\n",
    "dp = Descriptives(df_clean, categorical, linear, datetime)\n",
    "#dp.make_descriptives()\n",
    "#dp.make_plots()\n",
    "dp.categorical_counts()\n",
    "dp.linear_counts()\n",
    "dp.datetimehistplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## run t test for each 2x binary feature \n",
    "#\n",
    "\n",
    "independent = ['Race_simplified', 'Sex#',  'Diabetic#',  'Dead/Alive', 'Covid']\n",
    "\n",
    "indy2 = ['Race#','GenHealth#', 'AgeCategory', 'BMIcategory']\n",
    "\n",
    "\n",
    "ttest_dict = {'Dependent':[], 'Independent':[], 't-statistic':[], 't-p-value':[], 'chi2-statistic':[], 'chi2-p-value':[]}\n",
    "\n",
    "chi2_dict = {'Dependent':[], 'Independent':[], 'chi2-statistic':[], 'chi2-p-value':[]}\n",
    "for i in independent:\n",
    "\n",
    "    for c in df_clean_dummies.columns:\n",
    "        #print(c, i)\n",
    "        df_clean_dummies[c].dropna(inplace=True)\n",
    "\n",
    "        #print(df_clean_dummies['Dead/Alive'])\n",
    "        if (i==c):\n",
    "            continue\n",
    "        elif (i=='Sex#') | (i=='Race_simplified'):\n",
    "            #print(stats.ttest_ind(df_clean_dummies[df_clean_dummies[i]==2][c], df_clean_dummies[df_clean_dummies[i]==1][c]))\n",
    "            t_stat, p_val = stats.ttest_ind(df_clean_dummies[df_clean_dummies[i]==2][c], df_clean_dummies[df_clean_dummies[i]==1][c])\n",
    "            print(i, c, t_stat, p_val)\n",
    "            print(df_clean_dummies[df_clean_dummies[i]==2][c].mean(), df_clean_dummies[df_clean_dummies[i]==1][c].mean())\n",
    "            #print('\\n')\n",
    "\n",
    "        else:\n",
    "            #print(stats.ttest_ind(df_clean_dummies[df_clean_dummies[i]==1][c], df_clean_dummies[df_clean_dummies[i]==0][c]))\n",
    "            t_stat, p_val = stats.ttest_ind(df_clean_dummies[df_clean_dummies[i]==1][c], df_clean_dummies[df_clean_dummies[i]==0][c])\n",
    "            print(i, c, t_stat, p_val)\n",
    "            print(df_clean_dummies[df_clean_dummies[i]==1][c].mean(), df_clean_dummies[df_clean_dummies[i]==0][c].mean())\n",
    "\n",
    "        ttest_dict['Dependent'].append(c)\n",
    "        ttest_dict['Independent'].append(i)\n",
    "        ttest_dict['t-statistic'].append(t_stat)\n",
    "        ttest_dict['t-p-value'].append(p_val)\n",
    "\n",
    "##run chi2 test for each feature\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "        #print(df_clean_dummies['Dead/Alive'])\n",
    "        \n",
    "        #print(stats.chi2_contingency(pd.crosstab(df_clean_dummies[i], df_clean_dummies[c])))\n",
    "        chi2, p_val, dof, expected = stats.chi2_contingency(pd.crosstab(df_clean_dummies[i], df_clean_dummies[c]))\n",
    "        ttest_dict['chi2-statistic'].append(chi2)\n",
    "        ttest_dict['chi2-p-value'].append(p_val)\n",
    "        #print('\\n')\n",
    "\n",
    "exp = 'analysis of variance'\n",
    "ttest_df = pd.DataFrame(ttest_dict)\n",
    "ttest_df.to_csv(path + exp + '/ttest.csv')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19 = ((df_clean_dummies['year']==2019) | (df_clean_dummies['year']==2018))\n",
    "df_20 = ((df_clean_dummies['year']==2020)| (df_clean_dummies['year']==2021))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##anova\n",
    "\n",
    "##bartlett test of homogeneity of variance\n",
    "\n",
    "##run anova for each feature\n",
    "\n",
    "df_clean_dummies['GenHealth#']=df_clean_dummies['GenHealth#'].astype(int)\n",
    "\n",
    "anova_dict = {'Dependent':[], 'Independent':[], 'F-statistic':[], 'F-p-value':[], 'chi2-statistic':[], 'chi2-p-value':[]}\n",
    "\n",
    "bartlett_dict = {'Dependent':[], 'Independent':[], 'bartlett-statistic':[], 'bartlett-p-value':[]}\n",
    "independent = ['Race#','GenHealth#']\n",
    "\n",
    "for i in independent:\n",
    "        for c in df_clean_dummies.columns:\n",
    "                print(c, i)\n",
    "                df_clean_dummies[c].dropna(inplace=True)\n",
    "\n",
    "                list1 = df_clean_dummies[df_clean_dummies[i]==1][c]\n",
    "                list2 = df_clean_dummies[df_clean_dummies[i]==2][c]\n",
    "                list3 = df_clean_dummies[df_clean_dummies[i]==3][c]\n",
    "                list4 = df_clean_dummies[df_clean_dummies[i]==4][c]\n",
    "                list5 = df_clean_dummies[df_clean_dummies[i]==5][c]\n",
    "\n",
    "\n",
    "                if (i==c):\n",
    "                        continue\n",
    "                elif c=='Dead/Alive':\n",
    "                        continue\n",
    "                else:\n",
    "                        kwstat, kwpval = stats.kruskal(list1, list2, list3, list4, list5)\n",
    "                        bartlett_stat, p_val = stats.bartlett(list1, list2, list3, list4, list5)\n",
    "                        print('\\n')\n",
    "                        bartlett_dict['Dependent'].append(c)\n",
    "                        bartlett_dict['Independent'].append(i)\n",
    "                        bartlett_dict['bartlett-statistic'].append(bartlett_stat)\n",
    "                        bartlett_dict['bartlett-p-value'].append(p_val)\n",
    "                        anova_dict['Dependent'].append(c)\n",
    "                        anova_dict['Independent'].append(i)\n",
    "                        anova_dict['F-statistic'].append(kwstat)\n",
    "                        anova_dict['F-p-value'].append(kwpval)\n",
    "                        ##dunn post hoc test\n",
    "                        print(kwstat, kwpval)\n",
    "                        dunntest=sp.posthoc_dunn([list1, list2, list3, list4, list5], p_adjust = 'bonferroni')\n",
    "                        dunntest.to_csv(path + exp + '/dunn_' + c + '_' + i + '.csv')   \n",
    "                        print(dunntest)\n",
    "\n",
    "                        ##chi square test\n",
    "                        chi2, p_val, dof, expected = stats.chi2_contingency(pd.crosstab(df_clean_dummies[i], df_clean_dummies[c]))\n",
    "                        anova_dict['chi2-statistic'].append(chi2)\n",
    "                        anova_dict['chi2-p-value'].append(p_val)\n",
    "                        \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "bartlett_df = pd.DataFrame(bartlett_dict)\n",
    "bartlett_df.to_csv(path + exp + '/bartlett.csv')\n",
    "kw_df = pd.DataFrame(anova_dict)\n",
    "kw_df.to_csv(path + exp + '/kw.csv')\n",
    "\n",
    "\n",
    "#print(bartlett_df)\n",
    "print(kw_df)\n",
    "\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot variables by race\n",
    "\n",
    "fig  = plt.figure(figsize=(20, 20))\n",
    "print(df_clean['year'].value_counts())\n",
    "'''for v in categorical:\n",
    "    fig  = plt.figure(figsize=(16, 8))\n",
    "    y = 'year'\n",
    "    x = v\n",
    "    sns.countplot(x=y, data=df_clean, hue=x)\n",
    "    plt.show()\n",
    "#plt.tight_layout()\n",
    "\n",
    "dead_df = df_clean[df_clean['Dead/Alive'] == 'Yes']\n",
    "print(len(dead_df))\n",
    "for v in categorical:\n",
    "    fig  = plt.figure(figsize=(16, 8))\n",
    "    y = 'year'\n",
    "    x = v\n",
    "    sns.countplot(x=y, data=dead_df, hue=x)\n",
    "    plt.show()'''\"analysis of variance\"\n",
    "\n",
    "for v in categorical:\n",
    "    fig  = plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    sns.countplot(x=v, data=df_clean, hue='Race#')\n",
    "    plt.show()\n",
    "\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self, df, categorical, linear, datetime):\n",
    "        self.df = df\n",
    "        self.categorical = categorical\n",
    "        self.linear = linear\n",
    "        self.datetime = datetime\n",
    "        self.exp = 'classification'\n",
    "    \n",
    "    def logisticregression(self, target):\n",
    "        df = self.df\n",
    "        categorical = self.categorical\n",
    "        linear = self.linear\n",
    "        datetime = self.datetime\n",
    "        df = df.drop(columns=['date'])\n",
    "        df = pd.get_dummies(df, columns=categorical)\n",
    "        print(df.columns)\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.44, random_state=42)\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "        print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(logreg.score(X_train, y_train)))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        #print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        #confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                        cf_matrix.flatten()]\n",
    "        group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                             cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "        labels = [f'{v1} {v2} {v3}' for v1, v2, v3 in\n",
    "                  zip(group_names,group_counts,group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "        sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "        plt.savefig(path + self.exp + '/confusionmatrix.png')\n",
    "        plt.show()\n",
    "\n",
    "    def clustering(self):\n",
    "\n",
    "        ##unsupervised clustering\n",
    "        df = self.df\n",
    "        categorical = self.categorical\n",
    "        linear = self.linear\n",
    "        datetime = self.datetime\n",
    "        #df = df.drop(columns=['date'])\n",
    "        #df = pd.get_dummies(df, columns=categorical)\n",
    "        #print(df.columns)\n",
    "\n",
    "        pca = PCA(2)\n",
    " \n",
    "        #Transform the data\n",
    "        df = pca.fit_transform(df)\n",
    "        \n",
    "        df.shape\n",
    "\n",
    "        model = KMeans(n_clusters=5, random_state=42)\n",
    "        model.fit(df)\n",
    "        label = model.labels_\n",
    "        print(label)\n",
    "        #df['cluster'] = label\n",
    "        #print(df['cluster'].value_counts())\n",
    "        #print(df)\n",
    "        #df.to_csv(path + self.exp + '/cluster.csv')\n",
    "\n",
    "        u_labels = np.unique(label)\n",
    "    \n",
    "    #plotting the results:\n",
    "    \n",
    "        for i in u_labels:\n",
    "            print(label)\n",
    "            plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def randomf(self, target):\n",
    "        ##random forest\n",
    "\n",
    "        df_clean_dummies = self.df\n",
    "\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "        if target == 'Covid':\n",
    "            df_clean_dummies = df_clean_dummies.drop(columns=['year'])\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "        x = df_clean_dummies.drop(target, axis=1)\n",
    "\n",
    "        #print(x)\n",
    "        y = df_clean_dummies[target]\n",
    "\n",
    "        #print(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.44, random_state=42)\n",
    "\n",
    "        ##RANDOM SEARCH\n",
    "\n",
    "        '''n_estimators = [int(x) for x in np.linspace(start = 400, stop = 1500, num = 10)]\n",
    "                        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(1, 30, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                    'max_features': max_features,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "        rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, random_state=42, n_jobs = -1, \n",
    "                                    scoring = (\"accuracy\", 'recall', 'precision'), refit='recall', cv = 3, verbose=2, return_train_score=True)\n",
    "\n",
    "\n",
    "\n",
    "        rf_random.fit(X_train, y_train)\n",
    "\n",
    "        clf=rf_random\n",
    "        i = clf.best_index_\n",
    "        print('best index:', i)\n",
    "        best_precision = clf.cv_results_['mean_test_precision'][i]\n",
    "        best_recall = clf.cv_results_['mean_test_recall'][i]\n",
    "\n",
    "\n",
    "        print('Best score (accuracy): {}'.format(clf.best_score_))\n",
    "        print('Mean precision: {}'.format(best_precision))\n",
    "        print('Mean recall: {}'.format(best_recall))\n",
    "        print('Best parameters: {}'.format(clf.best_params_))\n",
    "        print('Best estimator: {}'.format(clf.best_estimator_))\n",
    "        print(rf_random.best_params_)\n",
    "        classifier = rf_random.best_estimator_'''\n",
    "\n",
    "        classifier = RandomForestClassifier(max_depth=10, n_estimators=400, random_state=42)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        print('Accuracy of random forest classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "                        \n",
    "\n",
    "        #if len(cnf) == 4:\n",
    "        #tn, fp, fn, tp = cnf_matrix\n",
    "        #print(cnf_matrix)\n",
    "        #specificity = tn / (tn + fp)\n",
    "        #print('Specificity: ', specificity)\n",
    "        #visualise confusion matrix\n",
    "        class_names=list(df_clean_dummies[target].unique()) # name  of classes\n",
    "        fig, ax = plt.subplots()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        # create heatmap\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='Spectral', fmt='g')\n",
    "        ax.xaxis.set_label_position(\"top\")\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path+self.exp+'/confusion_matrix_{}.png'.format(target), dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "        '''def confusion_matrix_scorer(clf, X, y):\n",
    "            y_pred = clf.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "                    'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "        cv_results = cross_validate(classifier, x, y, cv=5,\n",
    "                                    scoring=confusion_matrix_scorer)'''\n",
    "\n",
    "\n",
    "        sensitivity = make_scorer(recall_score, pos_label=1)\n",
    "        scores=cross_val_score(classifier, x, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "        se_scores=cross_val_score(classifier, x, y, cv=5, scoring=sensitivity)\n",
    "        print('sensitivity cross val:', se_scores)\n",
    "        print('sensitivity mean:', se_scores.mean())\n",
    "        print('sensitivity std:', se_scores.std())\n",
    "        precision_scores=cross_val_score(classifier, x, y, cv=5, scoring='precision')\n",
    "        print('precision cross val:', precision_scores)\n",
    "        print('precision mean:', precision_scores.mean())\n",
    "        print('precision std:', precision_scores.std())\n",
    "\n",
    "\n",
    "        importances = classifier.feature_importances_\n",
    "        feature_imp = pd.Series(importances, index = x.columns).sort_values(ascending = False)\n",
    "        #print(feature_imp.index[:10])\n",
    "        std = np.std([tree.feature_importances_ for tree in classifier.estimators_], axis=0)\n",
    "        #forest_importances = pd.Series(importances, index=xo)\n",
    "        fig, ax = plt.subplots(figsize=(20, 10), dpi=600)\n",
    "        #features = pd.Series(feature_imp.index[:10])\n",
    "        feature_imp.plot.bar(yerr=std, ax=ax)\n",
    "        #ax.set_title('Feature importance: {} Decision with {} X variables'.format(target.split('_')[1], xo[1][:2]), fontsize=16)\n",
    "        #ax.set_title(\"Feature importances using MDI\")\n",
    "        ax.set_ylabel('Importance', fontsize=16)\n",
    "        ax.set_xlabel('Features', fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "        #fig.tight_layout()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path+self.exp+'/feature_importance_{}.png'.format(target), dpi=1200, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "### run classification\n",
    "\n",
    "\n",
    "cl = Classification(df_clean_dummies, categorical, linear, datetime)\n",
    "#cl.logisticregression('GenHealth_Very good')\n",
    "#cl.clustering()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "targets = ['Race#', 'Race_simplified', 'Dead/Alive', 'GenHealth#', 'Covid']\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    cl.randomf(target)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_clean_dummies = pd.read_csv(path+'datasets'+'/df_clean_dummies.csv')\n",
    "\n",
    "\n",
    "\n",
    "##downsample \n",
    "\n",
    "#from sklearn.utils import resample\n",
    "#df_clean_dummies_white = resample(df_clean_dummies[df_clean_dummies['Race#'] == 1], replace=False, n_samples=len(df_clean_dummies[df_clean_dummies['Race#']==2]), random_state=42)\n",
    "\n",
    "mask = (df_clean_dummies['Race_simplified'] == 1)\n",
    "\n",
    "\n",
    "\n",
    "idx, = np.where(mask)\n",
    "\n",
    "df_clean_dummies = df_clean_dummies.loc[df_clean_dummies['Race#']!=1]\n",
    "\n",
    "\n",
    "df_clean_dummies.drop('Race_simplified', axis=1, inplace=True)\n",
    "\n",
    "x = df_clean_dummies.drop('Race#', axis=1)\n",
    "\n",
    "#print(x)\n",
    "y = df_clean_dummies['Race#']\n",
    "\n",
    "#print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.44, random_state=42)\n",
    "\n",
    "##RANDOM SEARCH\n",
    "\n",
    "'''n_estimators = [int(x) for x in np.linspace(start = 400, stop = 1500, num = 10)]\n",
    "                # Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 30, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 100, random_state=42, n_jobs = -1, \n",
    "                            scoring = (\"accuracy\", 'recall', 'precision'), refit='recall', cv = 3, verbose=2, return_train_score=True)\n",
    "\n",
    "\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "clf=rf_random\n",
    "i = clf.best_index_\n",
    "print('best index:', i)\n",
    "best_precision = clf.cv_results_['mean_test_precision'][i]\n",
    "best_recall = clf.cv_results_['mean_test_recall'][i]\n",
    "\n",
    "\n",
    "print('Best score (accuracy): {}'.format(clf.best_score_))\n",
    "print('Mean precision: {}'.format(best_precision))\n",
    "print('Mean recall: {}'.format(best_recall))\n",
    "print('Best parameters: {}'.format(clf.best_params_))\n",
    "print('Best estimator: {}'.format(clf.best_estimator_))\n",
    "print(rf_random.best_params_)\n",
    "classifier = rf_random.best_estimator_'''\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=10, n_estimators=400, random_state=42)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print('Accuracy of random forest classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "                \n",
    "\n",
    "#if len(cnf) == 4:\n",
    "#tn, fp, fn, tp = cnf_matrix\n",
    "#print(cnf_matrix)\n",
    "#specificity = tn / (tn + fp)\n",
    "#print('Specificity: ', specificity)\n",
    "#visualise confusion matrix\n",
    "class_names=[1, 2] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='Spectral', fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(path+'confusion_matrix_{}_{}.png'.format(target.split('_')[1], xo[1][:2]), dpi=300, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "plt.show()\n",
    "\n",
    "'''def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "cv_results = cross_validate(classifier, x, y, cv=5,\n",
    "                            scoring=confusion_matrix_scorer)'''\n",
    "\n",
    "\n",
    "sensitivity = make_scorer(recall_score, pos_label=1)\n",
    "scores=cross_val_score(classifier, x, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross validation accuracy:\", scores.mean())\n",
    "#sp_scores=cross_val_score(classifier, x, y, cv=5, scoring=specificity)\n",
    "#print('specificity cross val:', sp_scores)\n",
    "\n",
    "#print('specificity mean:', sp_scores.mean())\n",
    "#print('specificity std:', sp_scores.std())\n",
    "se_scores=cross_val_score(classifier, x, y, cv=5, scoring=sensitivity)\n",
    "print('sensitivity cross val:', se_scores)\n",
    "print('sensitivity mean:', se_scores.mean())\n",
    "print('sensitivity std:', se_scores.std())\n",
    "precision_scores=cross_val_score(classifier, x, y, cv=5, scoring='precision')\n",
    "print('precision cross val:', precision_scores)\n",
    "print('precision mean:', precision_scores.mean())\n",
    "print('precision std:', precision_scores.std())\n",
    "\n",
    "\n",
    "importances = classifier.feature_importances_\n",
    "feature_imp = pd.Series(importances, index = x.columns).sort_values(ascending = False)\n",
    "#print(feature_imp.index[:10])\n",
    "std = np.std([tree.feature_importances_ for tree in classifier.estimators_], axis=0)\n",
    "#forest_importances = pd.Series(importances, index=xo)\n",
    "fig, ax = plt.subplots(figsize=(20, 10), dpi=600)\n",
    "#features = pd.Series(feature_imp.index[:10])\n",
    "feature_imp.plot.bar(yerr=std, ax=ax)\n",
    "#ax.set_title('Feature importance: {} Decision with {} X variables'.format(target.split('_')[1], xo[1][:2]), fontsize=16)\n",
    "#ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel('Importance', fontsize=16)\n",
    "ax.set_xlabel('Features', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "#fig.tight_layout()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(path+'feature_importance_{}_{}.png'.format(target.split('_')[1], xo[1][:2]), dpi=1200, bbox_inches='tight', facecolor='white', transparent=False)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees=tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "df_clean_dummies = pd.read_csv(path+'datasets'+'/df_clean_dummies.csv')\n",
    "target = 'Race#'\n",
    "\n",
    "df_clean_dummies = df_clean_dummies.loc[df_clean_dummies['Race#']!=1]\n",
    "\n",
    "\n",
    "x = df_clean_dummies.drop(target, axis=1)\n",
    "#print(x)\n",
    "y = df_clean_dummies[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "trees.fit(X_train, y_train)\n",
    "y_pred = trees.predict(X_test)\n",
    "#print(y, X)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "specificity = confusion_matrix(y_test, y_pred)[0,0]/(confusion_matrix(y_test, y_pred)[0,0]+confusion_matrix(y_test, y_pred)[0,1])\n",
    "print('Specificity: ', specificity)\n",
    "##confisuon \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=16)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(path + target.split('_')[1] + '_' + x[0][:2] + '_simple_tree_confusion_matrix.png')\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(trees, \n",
    "                        feature_names=x.columns,\n",
    "                        fontsize=10,\n",
    "                        filled=True)\n",
    "plt.show()\n",
    "\n",
    "spec = make_scorer(recall_score, pos_label=0)\n",
    "sensitivity = make_scorer(recall_score, pos_label=1)\n",
    "scores=cross_val_score(trees, x, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross validation accuracy:\", scores.mean())\n",
    "sp_scores=cross_val_score(trees, x, y, cv=5, scoring=spec)\n",
    "print('specificity cross val:', sp_scores)\n",
    "\n",
    "print('specificity mean:', sp_scores.mean())\n",
    "print('specificity std:', sp_scores.std())\n",
    "se_scores=cross_val_score(trees, x, y, cv=5, scoring=sensitivity)\n",
    "print('sensitivity cross val:', se_scores)\n",
    "print('sensitivity mean:', se_scores.mean())\n",
    "print('sensitivity std:', se_scores.std())\n",
    "precision_scores=cross_val_score(trees, x, y, cv=5, scoring='precision')\n",
    "print('precision cross val:', precision_scores)\n",
    "print('precision mean:', precision_scores.mean())\n",
    "print('precision std:', precision_scores.std())\n",
    "\n",
    "def confusion_matrix_scorer(clf, x, y):\n",
    "        y_pred = clf.predict(X)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "                'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "cv_results = cross_validate(trees, x, y, cv=5,\n",
    "                                scoring=confusion_matrix_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Race#', ylabel='GenHealth#'>"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean_dummies = pd.read_csv(path+'datasets'+'/df_clean_dummies.csv')\n",
    "sns.barplot(x='Race#', y='BMI', data=df_clean_dummies)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.boxplot(x='Race#', y='GenHealth#', data=df_clean_dummies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe6768c439df95c45b1ef781494ca15f387e2cda4440aa40b48ca20f1d315678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
